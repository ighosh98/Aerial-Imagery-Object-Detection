The proposed architecture is a modification of the Faster R-CNN network.
Similar to the Faster R-CNN network, the proposed model contains 2 networks
built upon a base CNN: oriented RPN and R-CNN. The oriented RPN (Region
Proposal Network) proposes object regions having multiple orientations. RCNN
performs recognition of these proposals generated by the oriented RPN. The
model works by feeding an image to the base CNN to generate the feature map.
The feature map is then given as input to the oriented RPN which generates the
region proposals. Then, the feature map, along with the region proposals are fed
to the R-CNN, which determines the detected regions and by providing a
confidence score to each region proposed. There are certain differences between
the traditional RPN and the proposed oriented RPN. Unlike, the original RPN,
the oriented RPN generates boxes with multiple orientations instead of just
horizontal boxes.

The proposed approach employs ResNet-50 as the base CNN model to generate
the feature map, instead of VGG-Net. Residual neural networks utilize skip
connections to jump over some layers. Typical ResNet models are implemented
with double- or triple- layer skips that contain nonlinearities like ReLU and batch
normalization in between. The ResNet-50 model consists of 5 stages each with a
convolution and Identity block. Each convolution block has 3 convolution layers
and each identity block also has 3 convolution layers. The ResNet-50 has over 23
million trainable parameters.

The oriented RPN is built upon the ResNet-50 architecture to generate the
oriented boxes. In the oriented proposal scheme, a bounding box with 4 tuples
(x,y,w,h) is predicted. The tuple (x,y) represents the centre of the box and (w,h),
the width and height of the box. The concept of angle is implicitly used to
generate new (x,y,w,h) tuples or boxes. The orientations considered are: 0, pi/6,
pi/3, pi/2. The angle considered is the angle from the positive direction of x-axis
to the direction parallel to the long side of the oriented bounding box. Moreover,
the other two parameters (aspect ratio and scale) used in the traditional RPN are
adopted as well. We employ 9 aspect ratios [1, 1/2, 2, 1/3, 3, 1/4, 4, 1/5, 5] and 3
scales [3,6,12]. Through this scheme, 108 generated regions are obtained at each
position of the feature map. The two sibling output layers pred_bbox and
pred_score generate 432 outputs (108 × 4) and 216 scores (108 × 2), respectively.

IMPLEMENTATION DETAILS
The network is implemented in 2 phases – oriented RPN and RCNN.
RPN is trained first, and the trained weights of RPN is used to train the RCNN.
During both, RPN and RCNN training, the images are first resized to a fixed
dimension (800,800). The resizing of image is followed by rescaling the
coordinates of the object boxes present in the image. The rescaling is done by the
same sacle factor with which the image has been resized.
The image preprocess is followed by reading the annotation files. The coordinates
and categories are first read into a pandas dataframe. The required values are
extracted into an array and used as ground truth values for training. Following
functions are used to achieve the above tasks:

_preprocess(): Image is read and resized into (800,800)
read_file(): Annotation file is read into pandas dataframe
parse_(): Box Coordinates are scaled and stored in numpy arrays in the
format (xmin,ymin,xmax,ymax)
The resized image, along with the ground truth values are sent to minbatch()
(produce_batch() in case of RCNN) which generates the training dataset for the
RPN (and RCNN) network. The function performs several calls to different
functions to generate the tiles, labels and bounding boxes (in case of RPN). The
sequence of functions implemented in the minibatch() function are listed below:
Calculating the stride between feature map and the original image
Based on the stride, a sample set of anchor box is generated (108 boxes,
taking all the scales, aspect ratios and angles), which is then applied to all
the feature map pixels.
The anchor boxes are pruned, based on the overlap with the ground truth
boxes – segregating foregorund and background boxes
The deltas are calculated using the bbox_transform() function, which
converts the coordinates into the format – (xcenter, ycenter, width, height). The
values are nothing, but the deltas, calculated as the difference between the
ground truth boxes and generated anchor boxes. These deltas become the
targets during training.
Based on the overlap, labels are assigned to all the bounding boxes (as 1,0,-
1). Batch tiles are generated for each boxes, by padding the feature map
points into 3x3 matrices.
The batch tiles, boxes and labels so obtained are used to train the RPN network.
For RCNN training, the feature map also needs to be passed, which is fed into the
ROI Pooling class to generate the regions of interest from the feature map. The
network can be trained directly after the generation of training data, by calling
‘keras.model.fit()’ or an input generator can be used. The input generator is
defined to be compatible with the keras’ ‘fit_generator()’ function which yields
256 (or any defined BATCH_SIZE) batches of data at a time for the network to
train. To train the network over several days, a keras ‘ModelCheckPoint’ has been
used, which saves the best model (model with best accuracy at any point of
training) obtained so far. The model can thus, be trained on a subset of images,
and checkpointed, which again can be loaded and put to training on a different
subset of images.
