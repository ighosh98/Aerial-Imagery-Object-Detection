{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from keras.layers import Conv2D, TimeDistributed, Flatten, Dense, BatchNormalization\n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Layer\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import bbox_overlaps, bbox_transform,loss_cls, smoothL1, unmap, filter_boxes, clip_boxes, py_cpu_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):  \n",
    "    col = ['bottomLeftX','bottomLeftY','bottomRightX','bottomRightY','topRightX','topRightY','topLeftX','topLeftY','category','difficult']\n",
    "    dfr = pd.read_csv(filename,sep=\" \",names = col,index_col=None, header=None)\n",
    "    #print(dfr)\n",
    "    return dfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_anchors(img_path, anchors, pad_size=50):\n",
    "    im = Image.open(img_path)\n",
    "    w,h=im.size\n",
    "    a4im = Image.new('RGB',\n",
    "                    (w+2*pad_size, h+2*pad_size),   # A4 at 72dpi\n",
    "                    (255, 255, 255))  # White\n",
    "    a4im.paste(im, (pad_size,pad_size))  # Not centered, top-left corner\n",
    "    for a in anchors:\n",
    "        a=(a+pad_size).astype(int).tolist()\n",
    "        draw = ImageDraw.Draw(a4im)\n",
    "        draw.rectangle(a,outline=(255,0,0), fill=None)\n",
    "    return a4im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### define the ROI Pooling layer\n",
    "class RoIPooling(Layer):\n",
    "    def __init__(self, size=(7, 7)):\n",
    "        self.size = size\n",
    "        super(RoIPooling, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.shape = input_shape\n",
    "        super(RoIPooling, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        ind=K.reshape(inputs[2],(-1,))\n",
    "        x = K.tf.image.crop_and_resize(inputs[0], inputs[1], ind, self.size)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        a=input_shape[1][0]\n",
    "        b=self.size[0]\n",
    "        c=self.size[1]\n",
    "        d=input_shape[0][3]\n",
    "        return (a,b,c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## the RCNN model - change output shape based on number of dota classes\n",
    "def RCNN():\n",
    "    feature_map = Input(batch_shape=(None,None,None,2048)) # can use multiple feature maps (a map for every roi/or batch) \n",
    "    rois = Input(batch_shape=(None, 4))\n",
    "    ind = Input(batch_shape=(None, 1),dtype='int32')\n",
    "    p1 = RoIPooling()([feature_map, rois, ind])\n",
    "    flat1 = Flatten()(p1)\n",
    "    fc1 = Dense(units=1024,activation=\"relu\",name=\"fc2\")(flat1)\n",
    "\n",
    "    output_deltas = Dense(units = 4*15,activation=\"linear\",kernel_initializer=\"uniform\",name=\"deltas2\")(fc1)\n",
    "\n",
    "    output_scores = Dense(units = 1*15,activation=\"softmax\",kernel_initializer=\"uniform\",name=\"scores2\")(fc1)\n",
    "\n",
    "    model = Model(inputs=[feature_map, rois, ind],outputs=[output_scores,output_deltas])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rcnn = RCNN()\n",
    "model_rcnn.compile(optimizer='rmsprop', loss={'deltas2':smoothL1, 'scores2':'categorical_crossentropy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Prepare MINI Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_ratios = [1, 1/2, 2, 1/3, 3, 1/4, 4, 1/5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras_applications\\resnet50.py:263: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "FG_FRAC=.25\n",
    "FG_THRESH=.5\n",
    "BG_THRESH_HI=.5\n",
    "BG_THRESH_LO=.1\n",
    "model_resnet = ResNet50(include_top=False, input_shape = (800,800, 3)) \n",
    "rpn_model = load_model('rpnmodel0412.h5', custom_objects={'loss_cls': loss_cls,'smoothL1':smoothL1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchors(ratios, base_width, base_height,scales=np.asarray([3,6,12])):\n",
    "    \"\"\"\n",
    "    Generate anchor (reference) windows by enumerating aspect ratios X\n",
    "    scales wrt a reference (0, 0, w_stride-1, h_stride-1) window.\n",
    "    \"\"\"\n",
    "    base_anchor = np.array([0, 0, base_width-1, base_height-1])\n",
    "    ratio_anchors = _ratio_enum(base_anchor, ratios)\n",
    "    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales) for i in range(ratio_anchors.shape[0])]) #shape[0] gives ratio.size number of anchors\n",
    "    return anchors\n",
    "\n",
    "def _whctrs(anchor):\n",
    "    \"\"\"\n",
    "    Return width, height, x center, and y center for an anchor (window).\n",
    "    \"\"\"\n",
    "    w = anchor[2] - anchor[0] + 1\n",
    "    h = anchor[3] - anchor[1] + 1\n",
    "    x_ctr = anchor[0] + 0.5 * (w - 1)\n",
    "    y_ctr = anchor[1] + 0.5 * (h - 1)\n",
    "    return w, h, x_ctr, y_ctr\n",
    "\n",
    "def _mkanchors(ws, hs, x_ctr, y_ctr):\n",
    "    \"\"\"\n",
    "    Given a vector of widths (ws) and heights (hs) around a center\n",
    "    (x_ctr, y_ctr), output a set of anchors (windows).\n",
    "    \"\"\"\n",
    "\n",
    "    ws = ws[:, np.newaxis]\n",
    "    hs = hs[:, np.newaxis]\n",
    "    anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n",
    "                         y_ctr - 0.5 * (hs - 1),\n",
    "                         x_ctr + 0.5 * (ws - 1),\n",
    "                         y_ctr + 0.5 * (hs - 1)))\n",
    "    return anchors\n",
    "\n",
    "def _ratio_enum(anchor, ratios):\n",
    "    \"\"\"\n",
    "    Enumerate a set of anchors for each aspect ratio wrt an anchor.\n",
    "    \"\"\"\n",
    "\n",
    "    w, h, x_ctr, y_ctr = _whctrs(anchor)\n",
    "    size = w * h\n",
    "    size_ratios = size / ratios\n",
    "    ws = np.round(np.sqrt(size_ratios))\n",
    "    hs = np.round(ws * ratios)\n",
    "    anchors = _mkanchors(ws, hs, x_ctr, y_ctr) # center reamins the same\n",
    "    return anchors\n",
    "\n",
    "def _scale_enum(anchor, scales):\n",
    "    \"\"\"\n",
    "    Enumerate a set of anchors for each scale wrt an anchor.\n",
    "    \"\"\"\n",
    "\n",
    "    w, h, x_ctr, y_ctr = _whctrs(anchor)\n",
    "    ws = w * scales\n",
    "    hs = h * scales\n",
    "    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n",
    "    return anchors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_transform_inv(boxes, deltas):\n",
    "    if boxes.shape[0] == 0:\n",
    "        return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n",
    "\n",
    "    boxes = boxes.astype(deltas.dtype, copy=False)\n",
    "    #print('boxes.shape-', boxes.shape)\n",
    "    #print('deltas.shape - ', deltas.shape)\n",
    "    widths = abs(boxes[:, 2] - boxes[:, 0]) + 1.0\n",
    "    heights = abs(boxes[:, 3] - boxes[:, 1]) + 1.0\n",
    "    ctr_x = boxes[:, 0] + 0.5 * widths\n",
    "    ctr_y = boxes[:, 1] + 0.5 * heights\n",
    "    dx = deltas[:, 0::4]\n",
    "    dy = deltas[:, 1::4]\n",
    "    dw = deltas[:, 2::4]\n",
    "    dh = deltas[:, 3::4]\n",
    "    \n",
    "    inds = min(len(widths),len(dx))\n",
    "    \n",
    "    pred_ctr_x = dx[:inds]*widths[:inds,np.newaxis] + ctr_x[:inds, np.newaxis]\n",
    "    pred_ctr_y =  dy[:inds]*heights[:inds, np.newaxis] + ctr_y[:inds, np.newaxis]\n",
    "    pred_w = np.exp(dw[:inds])*widths[:inds, np.newaxis]\n",
    "    pred_h = np.exp(dh[:inds])*heights[:inds, np.newaxis]\n",
    "    \n",
    "    #print('pauseeeeeeee')\n",
    "    \n",
    "    pred_boxes = np.zeros((inds,4), dtype=deltas.dtype)\n",
    "    # x1\n",
    "    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n",
    "    # y1\n",
    "    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n",
    "    # x2\n",
    "    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n",
    "    # y2\n",
    "    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n",
    "\n",
    "    return pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_boxes(boxes, im_width, im_height):\n",
    "    \"\"\"\n",
    "    Clip boxes to image boundaries.\n",
    "    \"\"\"\n",
    "\n",
    "    # x1 >= 0\n",
    "    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_height - 1), 0)\n",
    "    # y1 >= 0\n",
    "    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_width - 1), 0)\n",
    "    # x2 < im_shape[1]\n",
    "    boxes[:, 2::4] = np.minimum(np.maximum(boxes[:, 2::4], im_height - 1), 0)\n",
    "    # y2 < im_shape[0]\n",
    "    boxes[:, 3::4] = np.minimum(np.maximum(boxes[:, 3::4], im_width - 1), 0)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(boxes, min_size):\n",
    "    \"\"\"Remove all boxes with any side smaller than min_size.\"\"\"\n",
    "    ws = (boxes[:, 2] - boxes[:, 0] + 1)*10\n",
    "    hs = (boxes[:, 3] - boxes[:, 1] + 1)*10\n",
    "    keep = np.where((ws >= min_size) & (hs >= min_size))[0]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_(filename,w_scale, h_scale):\n",
    "    df = read_file(filename)\n",
    "    width=0\n",
    "    height=0\n",
    "    list_of_widths = abs(df['topLeftX']-df['topRightX'])\n",
    "    list_of_hts = abs(df['topLeftY']-df['bottomLeftY'])\n",
    "    \n",
    "    category=[]\n",
    "    xmin=[]\n",
    "    ymin=[]\n",
    "    xmax=[]\n",
    "    ymax=[]\n",
    "    for i in df['category']:\n",
    "        category.append(i)\n",
    "    \n",
    "    for j in df['bottomLeftX']:\n",
    "        xmin.append(int(j)*(w_scale))\n",
    "    \n",
    "    for k in df['bottomLeftY']:\n",
    "        ymin.append(int(k)*(h_scale))\n",
    "    \n",
    "    for l in df['topRightX']:\n",
    "        xmax.append(int(l)*(w_scale))\n",
    "    \n",
    "    for m in df['topRightY']:\n",
    "        ymax.append(int(m)*(h_scale))\n",
    "        \n",
    "    gt_boxes=[list(box) for box in zip(xmin,ymin,xmax,ymax)]\n",
    "    \n",
    "    return category, np.asarray(gt_boxes, np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_batch(filepath, gt_boxes, category, img):\n",
    "    \n",
    "    #img = cv2.imread(filepath)\n",
    "    img_width = np.shape(img)[1]#*scale[1]\n",
    "    img_height = np.shape(img)[0]#*scale[0]\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    print('img shape inside produce batch - ', img.shape)\n",
    "    feature_map = model_resnet.predict(img)\n",
    "    #print(\"feature map- \", feature_map.shape)\n",
    "    height = np.shape(feature_map)[1]\n",
    "    width = np.shape(feature_map)[2]\n",
    "    num_feature_map = width*height\n",
    "    w_stride = img_width/width\n",
    "    h_stride = img_height/height\n",
    "    #generate base anchors according output stride.\n",
    "    #base anchors are 9 anchors wrt a tile (0,0,w_stride-1,h_stride-1)\n",
    "    base_anchors = generate_anchors(anchor_ratios,w_stride,h_stride)\n",
    "    #slice tiles according to image size and stride.\n",
    "    #each 1x1x1532 feature map is mapping to a tile.\n",
    "    shift_x = np.arange(0, width) * w_stride\n",
    "    shift_y = np.arange(0, height) * h_stride\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(),\n",
    "                            shift_y.ravel())).transpose()\n",
    "    #apply base anchors to all tiles, to have a num_feature_map*9 anchors.\n",
    "    all_anchors = (base_anchors.reshape((1, 27, 4)) +\n",
    "                    shifts.reshape((1, num_feature_map, 4)).transpose((1, 0, 2)))\n",
    "    #print(all_anchors.shape)\n",
    "    total_anchors = num_feature_map*27\n",
    "    all_anchors = all_anchors.reshape((total_anchors, 4))\n",
    "    \n",
    "    # feed feature map to pretrained RPN model, get proposal labels and bboxes.\n",
    "    \n",
    "    res = rpn_model.predict(feature_map)\n",
    "    \n",
    "    scores = np.array(res[1])\n",
    "    deltas = np.array(res[0])\n",
    "    \n",
    "    scores = scores.reshape(-1,1)\n",
    "    \n",
    "    deltas = np.reshape(deltas,(-1,4))\n",
    "    \n",
    "    # proposals transform to bbox values (x1, y1, x2, y2)\n",
    "    proposals = bbox_transform_inv(all_anchors, deltas)\n",
    "    # (x1,y1,x2,y2)\n",
    "    proposals = clip_boxes(proposals, img_width,img_height)\n",
    "    # remove small boxes, here threshold is 40 pixel\n",
    "    keep = filter_boxes(proposals, 40)#remove boxes with size greater than this returns the indices\n",
    "    proposals = proposals[keep-1, :]\n",
    "    scores = scores[np.array(keep-1)] ############################\n",
    "    #scores = scores[keep]\n",
    "\n",
    "    # sort socres and only keep top 6000.\n",
    "    pre_nms_topN=6000\n",
    "    order = scores.ravel().argsort()[::-1]\n",
    "    if pre_nms_topN > 0:\n",
    "        order = order[:pre_nms_topN]\n",
    "    proposals = proposals[order, :]\n",
    "    scores = scores[order]\n",
    "    # apply NMS to to 6000, and then keep top 300\n",
    "    post_nms_topN=300\n",
    "    keep = py_cpu_nms(np.hstack((proposals, scores)), 0.7)\n",
    "    \n",
    "    if post_nms_topN > 0:\n",
    "        keep = keep[:post_nms_topN]\n",
    "    \n",
    "    # add gt_boxes to proposals.\n",
    "    proposals=np.vstack( (proposals, gt_boxes) )\n",
    "    # calculate overlaps of proposal and gt_boxes\n",
    "    overlaps = bbox_overlaps(proposals, gt_boxes)\n",
    "    gt_assignment = overlaps.argmax(axis=1)\n",
    "    max_overlaps = overlaps.max(axis=1)\n",
    "\n",
    "    # sub sample\n",
    "    fg_inds = np.where(max_overlaps >= FG_THRESH)[0]\n",
    "    fg_rois_per_this_image = min(int(BATCH*FG_FRAC), fg_inds.size) #2\n",
    "    # Sample foreground regions without replacement\n",
    "    if fg_inds.size > 0:\n",
    "        fg_inds = npr.choice(fg_inds, size=fg_rois_per_this_image, replace=False)\n",
    "    bg_inds = np.where((max_overlaps/10 < BG_THRESH_HI) &\n",
    "                       (max_overlaps/10 >= BG_THRESH_LO))[0]\n",
    "    bg_rois_per_this_image = BATCH - fg_rois_per_this_image\n",
    "    bg_rois_per_this_image = min(bg_rois_per_this_image, bg_inds.size)\n",
    "    # Sample background regions without replacement\n",
    "    if bg_inds.size > 0:\n",
    "        bg_inds = npr.choice(bg_inds, size=bg_rois_per_this_image, replace=False)\n",
    "    # The indices that we're selecting (both fg and bg)\n",
    "    keep_inds = np.append(fg_inds, bg_inds)\n",
    "    # Select sampled values from various arrays:\n",
    "    rois = proposals[keep_inds]\n",
    "    gt_rois = gt_boxes[gt_assignment[keep_inds]]\n",
    "    targets = bbox_transform(rois-0.5, gt_rois)#input rois rois-1 to introduce randomness\n",
    "    print(targets.any())\n",
    "    rois_num = targets.shape[0]\n",
    "    batch_box = np.zeros((rois_num, 15, 4))\n",
    "    for i in range(rois_num):\n",
    "        batch_box[i, category] = targets[i]\n",
    "    batch_box = np.reshape(batch_box, (rois_num, -1))\n",
    "    # get gt category\n",
    "    batch_categories = np.zeros((rois_num, 15, 1))\n",
    "    for i in range(rois_num):\n",
    "        batch_categories[i, category] = 1\n",
    "    batch_categories = np.reshape(batch_categories, (rois_num, -1))\n",
    "    \n",
    "    return rois, batch_box, batch_categories, feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### prepare data for training\n",
    "img_path = r'F:\\DOTA (Dataset)\\Training Set\\part 1\\test images'\n",
    "anno_path = r'F:\\DOTA (Dataset)\\Training Set\\part 1\\test annos'\n",
    "\n",
    "import glob\n",
    "#from multiprocessing import Process, Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_(file):\n",
    "    img = cv2.imread(file)\n",
    "    dim = (800,800)\n",
    "    img_ = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    scale_w = img_.shape[0]/img.shape[0]\n",
    "    scale_h = img_.shape[1]/img.shape[1]\n",
    "    return img_,scale_w, scale_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    \n",
    "    batch_rois=[]\n",
    "    batch_featuremap_inds=[]\n",
    "    batch_categories=[]\n",
    "    batch_bboxes=[]\n",
    "    fc_index=0 \n",
    "    \n",
    "    while 1:\n",
    "        for filename in os.listdir(anno_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                try:\n",
    "                    file = anno_path + '\\\\'\n",
    "                    filepath = img_path + '\\\\' + filename[:-4]+ '.png'\n",
    "                    img, scw, sch = preprocess_(filepath)\n",
    "                    print('image shape is: ', img.shape)\n",
    "                    category, gt_boxes = parse_(file+filename,scw,sch)\n",
    "                    if len(gt_boxes)==0:\n",
    "                        continue\n",
    "                    rois, bboxes, categories, feature_map = produce_batch(filepath, gt_boxes, category, img)\n",
    "                except Exception:\n",
    "                    print('parse label or produce batch failed')\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "                if len(rois)<=0:\n",
    "                    continue\n",
    "                for i in range(len(rois)):\n",
    "                    batch_rois.append(rois[i])\n",
    "                    batch_featuremap_inds.append(i)\n",
    "                    batch_categories.append(categories[i])\n",
    "                    batch_bboxes.append(bboxes[i])\n",
    "                \n",
    "                a=feature_map\n",
    "                b=np.asarray(batch_rois)\n",
    "                c=np.asarray(batch_featuremap_inds)\n",
    "                d=np.asarray(batch_categories)\n",
    "                e=np.asarray(batch_bboxes)\n",
    "                f=np.zeros((len(rois),a.shape[1],a.shape[2],a.shape[3]))\n",
    "                f[0]=feature_map[0]\n",
    "                yield [f,b,c], [d,e]\n",
    "                batch_rois=[]\n",
    "                batch_featuremap_inds=[]\n",
    "                batch_categories=[]\n",
    "                batch_bboxes=[]\n",
    "                #fc_index=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_rois=[]\n",
    "batch_featuremap_inds=[]\n",
    "batch_categories=[]\n",
    "batch_bboxes=[]\n",
    "featuremap = []\n",
    "fc_index=0\n",
    "category = 0\n",
    "for filename in os.listdir(anno_path):\n",
    "    file = anno_path + '\\\\'\n",
    "    filepath = img_path + '\\\\' + filename[:-4]+ '.png'\n",
    "    img, scw, sch = preprocess_(filepath)\n",
    "    \n",
    "    _, gt_boxes = parse_(file+filename,scw,sch)\n",
    "    \n",
    "    \n",
    "    if len(gt_boxes)==0:\n",
    "        continue\n",
    "    \n",
    "    rois, bboxes, categories, feature_map = produce_batch(filepath, gt_boxes, category, img)\n",
    "    category = category+1\n",
    "    #tiles, labels, bboxes = minibatch(filepath, gt_boxes, scale)\n",
    "    for i in range(len(rois)):\n",
    "        featuremap.append(feature_map)\n",
    "        batch_rois.append(rois[i])\n",
    "        batch_featuremap_inds.append(i)\n",
    "        batch_categories.append(categories[i])\n",
    "        batch_bboxes.append(bboxes[i])\n",
    "\n",
    "a = np.asarray(featuremap)\n",
    "b = np.asarray(batch_rois)\n",
    "c = np.asarray(batch_featuremap_inds)\n",
    "d = np.asarray(batch_categories)\n",
    "e = np.asarray(batch_bboxes)\n",
    "f = a#np.zeros((len(rois),a.shape[1],a.shape[2],a.shape[3]))\n",
    "#f[0] = featuremap[0]\n",
    "\n",
    "if not a.any() or not b.any() or not c.any() or not d.any() or not e.any() or not f.any():\n",
    "        print(\"empty array found.\")\n",
    "print('a.any() - ', a.any(), 'b.any() - ', b.any(), 'c.any() - ', c.any(), 'd.any() - ', d.any(), 'e.any() - ', e.any(), \n",
    "     'f.any() - ', f.any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 25, 25, 2048), (487, 4), (487, 1), (487, 15), (487, 60))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape, b.shape, c.shape, d.shape, e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 25, 25, 2048), (3, 4), (3,), (3, 15), (3, 60))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part of a previous test\n",
    "f.shape, b.shape, c.shape, d.shape, e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.7089 - scores2_loss: 2.7081 - deltas2_loss: 8.0270e-04\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 462ms/step - loss: 2.7028 - scores2_loss: 2.7021 - deltas2_loss: 6.9402e-04\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 1s 467ms/step - loss: 2.6985 - scores2_loss: 2.6979 - deltas2_loss: 6.2454e-04\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 1s 472ms/step - loss: 2.6949 - scores2_loss: 2.6943 - deltas2_loss: 5.7156e-04\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 1s 467ms/step - loss: 2.6916 - scores2_loss: 2.6911 - deltas2_loss: 5.2818e-04\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 1s 466ms/step - loss: 2.6887 - scores2_loss: 2.6882 - deltas2_loss: 4.9124e-04\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 1s 494ms/step - loss: 2.6859 - scores2_loss: 2.6855 - deltas2_loss: 4.5903e-04\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 2s 505ms/step - loss: 2.6833 - scores2_loss: 2.6829 - deltas2_loss: 4.3046e-04\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 1s 500ms/step - loss: 2.6808 - scores2_loss: 2.6804 - deltas2_loss: 4.0482e-04\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 490ms/step - loss: 2.6784 - scores2_loss: 2.6781 - deltas2_loss: 3.8162e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d55c6d40f0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rcnn.fit([f,b,c], [d,e], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Using input generator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='./rcnn_weights_2.hdf5', monitor='loss', verbose=1, save_best_only=True)\n",
    "model_rcnn.fit_generator(train_generator(), steps_per_epoch=30, epochs=10, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_img_path = r'C:\\Users\\user\\Desktop\\Abhilash\\4th year\\DOP\\P1053.png'\n",
    "pred_anno_path = r'F:\\DOTA (Dataset)\\Training Set\\part 1\\test annos'\n",
    "pred_img_path = r'F:\\DOTA (Dataset)\\Training Set\\part 1\\test images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectMax(categories):\n",
    "    classes = ['plane','ship','storage tank','baseball diamond','tennis court','basketball court',\n",
    "               'ground track field','harbor','bridge','large vehicle','small vehicle','helicopter',\n",
    "               'roundabout','soccer ball field','swimming pool']\n",
    "    category = []\n",
    "    for i in len(categories):\n",
    "        ind = np.argmax(categories[i])\n",
    "        category.append(classes[ind[0]])\n",
    "    return np.asarray(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.text as text\n",
    "\n",
    "def predictImages(imgfile, model_rcnn,rois):\n",
    "    #testimg = cv2.imread(filename)\n",
    "    testimg = np.expand_dims(imgfile, axis=0)\n",
    "    pred_resnet = model_resnet.predict(testimg)\n",
    "    pred_rpn = rpn_model.predict(pred_resnet)\n",
    "    \n",
    "    pred_rpn_scores = np.asarray(pred_rpn[1])\n",
    "    pred_rpn_deltas = np.asarray(pred_rpn[0])\n",
    "    \n",
    "    pred_rpn_deltas = pred_rpn_deltas.reshape(-1,4)\n",
    "    pred_rpn_scores = pred_rpn_scores.reshape(-1,1)\n",
    "    \n",
    "    inds = [i for i in range(0,len(rois))]\n",
    "    pred_rcnn = model_rcnn.predict([pred_resnet, rois, np.asarray(inds)])\n",
    "    \n",
    "    pred_deltas = np.asarray(pred_rcnn[1]).reshape(-1,4)\n",
    "    pred_categories = np.asarray(pred_rcnn[0]).reshape(-1,1)\n",
    "    \n",
    "    boxes = bbox_transform_inv(rois, pred_deltas)\n",
    "    pred_categories = selectMax(pred_categories)\n",
    "    return pred_categories, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPredicted(im, deltas, categories):\n",
    "    #im = np.array(Image.open(img), dtype=np.uint8)\n",
    "    #im = cv2.imread(img)\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111,)\n",
    "    category = categories\n",
    "    gt_boxes = deltas\n",
    "    ax1.imshow(im)\n",
    "    #gt_boxes = gt_boxes[:4]\n",
    "    for r in range(0,len(gt_boxes)):\n",
    "        ax1.add_patch(patches.Rectangle((gt_boxes[r,0],gt_boxes[r,1]),gt_boxes[r,2]-gt_boxes[r,0],gt_boxes[r,3]-gt_boxes[r,1],1,linewidth=1,edgecolor='r',facecolor='none'))\n",
    "        plt.text(gt_boxes[r,0],gt_boxes[r,1],category[r], color = 'black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTest():\n",
    "    count=0\n",
    "    for files in os.listdir(pred_img_path):\n",
    "        imgfile = pred_img_path + '\\\\' + files\n",
    "        anno = pred_anno_path + '\\\\' + files[:-4] + '.txt'\n",
    "        img_, scw, sch = preprocess_(imgfile)\n",
    "        categories, boxes = predictImages(img_,model_rcnn,rois)\n",
    "        plotPredicted(img_, boxes, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(boxes,gt_boxes):\n",
    "    avg = 0.0\n",
    "    for i in range(0,len(boxes)):\n",
    "        box_area = (boxes[i, 2] - boxes[i, 0] + 1) * (boxes[i, 3] - boxes[i, 1] + 1)\n",
    "        gt_area = (gt_boxes[i, 2] - gt_boxes[i, 0] + 1) * (gt_boxes[i, 3] - gt_boxes[i, 1] + 1)\n",
    "        iw = (min(gt_boxes[i, 2], boxes[i, 2]) - max(gt_boxes[i, 0], boxes[i, 0]) + 1)\n",
    "        ih = (min(gt_boxes[i, 3], boxes[i, 3]) - max(gt_boxes[i, 1], boxes[i, 1]) + 1)\n",
    "        ua = box_area+gt_area-(iw*ih)\n",
    "        avg = avg + ((iw*ih)/ua)\n",
    "    avg = avg/len(boxes)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "value =[]\n",
    "count=0\n",
    "for files in os.listdir(pred_img_path):\n",
    "    imgfile = pred_img_path + '\\\\' + files\n",
    "    anno = pred_anno_path + '\\\\' + files[:-4] + '.txt'\n",
    "    img_, scw, sch = preprocess_(imgfile)\n",
    "    categories, boxes = predictImages(img_,model_rcnn,rois)\n",
    "    cats, gt_boxes = parse_(file+filename,scw,sch)\n",
    "    value.append(calc(boxes,gt_boxes)\n",
    "    count = count+1\n",
    "value = value/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rcnn.save('rcnn_weightsUpdated.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
